"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[2508],{71:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"module-three/05-module-three-glossary","title":"Module 3 Glossary of Terms","description":"This glossary provides definitions for key terms and concepts introduced in Module 3.","source":"@site/docs/03-module-three/05-module-three-glossary.md","sourceDirName":"03-module-three","slug":"/module-three/05-module-three-glossary","permalink":"/Physical-AI-And-Humanoid-Robotics-Book/docs/module-three/05-module-three-glossary","draft":false,"unlisted":false,"editUrl":"https://github.com/MinalSaleem/Physical-AI-And-Humanoid-Robotics-Book/tree/main/book/docs/03-module-three/05-module-three-glossary.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"05-module-three-glossary","title":"Module 3 Glossary of Terms","sidebar_label":"Glossary","sidebar_position":5}}');var t=i(4848),a=i(8453);const s={id:"05-module-three-glossary",title:"Module 3 Glossary of Terms",sidebar_label:"Glossary",sidebar_position:5},r="Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122) - Glossary",l={},d=[{value:"A",id:"a",level:2},{value:"B",id:"b",level:2},{value:"C",id:"c",level:2},{value:"D",id:"d",level:2},{value:"F",id:"f",level:2},{value:"I",id:"i",level:2},{value:"L",id:"l",level:2},{value:"N",id:"n",level:2},{value:"O",id:"o",level:2},{value:"P",id:"p",level:2},{value:"R",id:"r",level:2},{value:"S",id:"s",level:2},{value:"T",id:"t",level:2},{value:"V",id:"v",level:2}];function c(e){const n={h1:"h1",h2:"h2",header:"header",p:"p",strong:"strong",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-3-the-ai-robot-brain-nvidia-isaac---glossary",children:"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122) - Glossary"})}),"\n",(0,t.jsx)(n.p,{children:"This glossary provides definitions for key terms and concepts introduced in Module 3."}),"\n",(0,t.jsx)(n.h2,{id:"a",children:"A"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Action (ROS 2)"}),": A high-level, asynchronous communication mechanism in ROS 2 for long-running tasks, allowing clients to send goals, receive feedback, and eventually get results."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Advanced Perception"}),": Techniques that go beyond basic sensor readings to create a rich, actionable understanding of the environment, often involving sensor fusion, semantic segmentation, and 3D reconstruction."]}),"\n",(0,t.jsx)(n.h2,{id:"b",children:"B"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Behavioral Cloning (BC)"}),": An imitation learning technique where an agent learns to mimic expert demonstrations by directly mapping observations to actions, typically using supervised learning."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Bipedal Humanoid Movement"}),": Complex locomotion capabilities of two-legged robots, involving challenges in maintaining balance, gait generation, and coordinating many degrees of freedom."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Bundle Adjustment"}),": An optimization technique used in VSLAM to refine the estimated camera poses and 3D map points by minimizing the reprojection error of observed features."]}),"\n",(0,t.jsx)(n.h2,{id:"c",children:"C"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Costmaps (Nav2)"}),": Environmental representations used by Nav2 to indicate areas that are safe, occupied by obstacles, or uncertain, guiding a robot's path planning."]}),"\n",(0,t.jsx)(n.h2,{id:"d",children:"D"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Deep Learning for Perception"}),": Application of deep neural networks to extract high-level features and understanding from raw sensor data (e.g., image, LiDAR point clouds) for tasks like object detection, segmentation, and classification."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Direct Methods (VSLAM)"}),": VSLAM techniques that use pixel intensity values directly (without explicit feature extraction) to estimate camera motion and map structure."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Domain Randomization"}),": A technique used in simulation (e.g., Isaac Sim) to randomize various aspects of the environment (lighting, textures, object properties) during data collection to improve the generalization of AI models to the real world."]}),"\n",(0,t.jsx)(n.h2,{id:"f",children:"F"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Feature-based Methods (VSLAM)"}),": VSLAM techniques that detect and track distinct visual features (e.g., corners, edges) in camera images to estimate camera motion and build a map."]}),"\n",(0,t.jsx)(n.h2,{id:"i",children:"I"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Imitation Learning (IL)"}),": A training methodology where an agent learns a task by observing and mimicking expert demonstrations, often used when defining a reward function for RL is difficult."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Inverse Reinforcement Learning (IRL)"}),": A technique used in imitation learning to infer the underlying reward function that explains observed expert behavior."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS"}),": A collection of hardware-accelerated ROS 2 packages developed by NVIDIA, optimizing critical robotics tasks like perception, navigation, and manipulation using NVIDIA GPUs."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Isaac Sim (NVIDIA Isaac Sim)"}),": A scalable robotics simulation application and synthetic data generation tool built on NVIDIA Omniverse, providing photorealistic environments for AI training."]}),"\n",(0,t.jsx)(n.h2,{id:"l",children:"L"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"LiDAR (Light Detection and Ranging)"}),": A remote sensing technology that uses pulsed laser light to measure distances, creating detailed 3D point clouds of the environment, crucial for mapping and navigation."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Loop Closure (VSLAM)"}),": The process in VSLAM of detecting when a robot returns to a previously visited location, used to correct accumulated error in the map and trajectory, ensuring global consistency."]}),"\n",(0,t.jsx)(n.h2,{id:"n",children:"N"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Nav2"}),": The ROS 2 navigation stack, providing a modular framework for autonomous mobile robot navigation, including global and local path planning, obstacle avoidance, and control."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"NVIDIA Omniverse"}),": A platform for connecting and building 3D tools and applications, enabling virtual collaboration and physically accurate simulation, upon which Isaac Sim is built."]}),"\n",(0,t.jsx)(n.h2,{id:"o",children:"O"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Object Detection and Tracking"}),": Identifying and localizing specific objects within an environment (detection) and continuously monitoring their position and motion over time (tracking)."]}),"\n",(0,t.jsx)(n.h2,{id:"p",children:"P"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Path Planning"}),": The process of finding a collision-free path for a robot from a start location to a goal location, often involving global and local planners."]}),"\n",(0,t.jsx)(n.h2,{id:"r",children:"R"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Reinforcement Learning (RL)"}),": A machine learning paradigm where an agent learns to make decisions by performing actions in an environment to maximize a cumulative reward signal through trial and error."]}),"\n",(0,t.jsx)(n.h2,{id:"s",children:"S"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Semantic Segmentation"}),': A computer vision technique that classifies each pixel in an image or point in a point cloud with a specific category label (e.g., "road," "person," "sky"), providing a high-level understanding of the scene.']}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sensor Fusion"}),": Combining data from multiple disparate sensors to achieve a more accurate, robust, and complete understanding of an environment or system state than would be possible with individual sensors."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sim-to-Real Transfer"}),": The process of training an AI model or robot control policy in a simulated environment and then effectively deploying it on a real physical robot."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Synthetic Data Generation (SDG)"}),": The creation of artificial data in simulation, often with ground truth labels, used to train AI models when real-world data collection is expensive, dangerous, or impractical."]}),"\n",(0,t.jsx)(n.h2,{id:"t",children:"T"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Transfer Learning"}),": A machine learning technique where a model trained on one task is reused as the starting point for a model on a second, related task, often to accelerate training or improve performance with limited data."]}),"\n",(0,t.jsx)(n.h2,{id:"v",children:"V"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"VSLAM (Visual Simultaneous Localization and Mapping)"}),": A technique that allows a robot to concurrently estimate its own position and orientation within an environment while simultaneously building a map of that environment, using visual sensor data from cameras."]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>r});var o=i(6540);const t={},a=o.createContext(t);function s(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);