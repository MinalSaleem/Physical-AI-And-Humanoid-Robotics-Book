"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[6548],{4785:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-four/05-module-four-glossary","title":"Module 4 Glossary of Terms","description":"This glossary provides definitions for key terms and concepts introduced in Module 4.","source":"@site/docs/04-module-four/05-module-four-glossary.md","sourceDirName":"04-module-four","slug":"/module-four/05-module-four-glossary","permalink":"/Physical-AI-And-Humanoid-Robotics-Book/docs/module-four/05-module-four-glossary","draft":false,"unlisted":false,"editUrl":"https://github.com/MinalSaleem/Physical-AI-And-Humanoid-Robotics-Book/tree/main/book/docs/04-module-four/05-module-four-glossary.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"05-module-four-glossary","title":"Module 4 Glossary of Terms","sidebar_label":"Glossary","sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4: Capstone Project","permalink":"/Physical-AI-And-Humanoid-Robotics-Book/docs/module-four/04-capstone-autonomous-humanoid"}}');var s=o(4848),t=o(8453);const r={id:"05-module-four-glossary",title:"Module 4 Glossary of Terms",sidebar_label:"Glossary",sidebar_position:5},a="Module 4: Vision-Language-Action (VLA) - Glossary",l={},d=[{value:"A",id:"a",level:2},{value:"C",id:"c",level:2},{value:"H",id:"h",level:2},{value:"L",id:"l",level:2},{value:"N",id:"n",level:2},{value:"O",id:"o",level:2},{value:"P",id:"p",level:2},{value:"R",id:"r",level:2},{value:"S",id:"s",level:2},{value:"V",id:"v",level:2}];function c(e){const n={h1:"h1",h2:"h2",header:"header",p:"p",strong:"strong",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"module-4-vision-language-action-vla---glossary",children:"Module 4: Vision-Language-Action (VLA) - Glossary"})}),"\n",(0,s.jsx)(n.p,{children:"This glossary provides definitions for key terms and concepts introduced in Module 4."}),"\n",(0,s.jsx)(n.h2,{id:"a",children:"A"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Action (ROS 2)"}),": A structured communication mechanism in ROS 2 designed for long-running, goal-oriented tasks, providing goal requests, feedback during execution, and final results."]}),"\n",(0,s.jsx)(n.h2,{id:"c",children:"C"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Cognitive Planning"}),": The process of translating high-level, often abstract, human intentions or goals into a concrete, executable sequence of steps or actions for a robot, often facilitated by Large Language Models."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Computer Vision"}),': A field of artificial intelligence that enables computers and robotic systems to "see," interpret, and understand visual information from the world (e.g., images, video).']}),"\n",(0,s.jsx)(n.h2,{id:"h",children:"H"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Human-Robot Interaction (HRI)"}),": The study and design of interfaces and interactions between humans and robots, aiming for intuitive, effective, and safe collaboration."]}),"\n",(0,s.jsx)(n.h2,{id:"l",children:"L"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Large Language Model (LLM)"}),": An artificial intelligence model capable of understanding, processing, and generating human-like text, utilized in robotics for tasks like cognitive planning, natural language understanding, and decision-making."]}),"\n",(0,s.jsx)(n.h2,{id:"n",children:"N"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Natural Language Understanding (NLU)"}),": A subfield of AI that focuses on enabling computers to understand the meaning and intent of human language, crucial for interpreting voice commands."]}),"\n",(0,s.jsx)(n.h2,{id:"o",children:"O"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"OpenAI Whisper"}),": A powerful, general-purpose speech recognition model developed by OpenAI that accurately transcribes spoken language into text, supporting various languages and robust to noise and accents."]}),"\n",(0,s.jsx)(n.h2,{id:"p",children:"P"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Prompt Engineering"}),": The process of carefully designing and refining input prompts to Large Language Models to elicit desired outputs, particularly important for guiding LLMs in robotics tasks like planning."]}),"\n",(0,s.jsx)(n.h2,{id:"r",children:"R"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Robot Manipulation"}),": The ability of a robot to physically interact with objects in its environment, including grasping, moving, and placing them, often requiring precise control and perception."]}),"\n",(0,s.jsx)(n.h2,{id:"s",children:"S"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Speech-to-Text (STT)"}),": Technology that converts audio input (spoken language) into written text, a foundational component for voice-controlled robotic systems."]}),"\n",(0,s.jsx)(n.h2,{id:"v",children:"V"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Vision-Language-Action (VLA)"}),": An interdisciplinary field combining computer vision (how a robot sees), natural language processing (how a robot understands/communicates), and robot control (how a robot acts) to enable intelligent and versatile robot behavior."]})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>a});var i=o(6540);const s={},t=i.createContext(s);function r(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);