"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[1021],{2884:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>p,frontMatter:()=>r,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"module-four/02-voice-to-action-whisper","title":"Voice-to-Action: Using OpenAI Whisper for Voice Commands Generation","description":"Introduction","source":"@site/docs/04-module-four/02-voice-to-action-whisper.md","sourceDirName":"04-module-four","slug":"/module-four/02-voice-to-action-whisper","permalink":"/Physical-AI-And-Humanoid-Robotics-Book/docs/module-four/02-voice-to-action-whisper","draft":false,"unlisted":false,"editUrl":"https://github.com/MinalSaleem/Physical-AI-And-Humanoid-Robotics-Book/tree/main/book/docs/04-module-four/02-voice-to-action-whisper.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"02-voice-to-action-whisper","title":"Voice-to-Action: Using OpenAI Whisper for Voice Commands Generation","sidebar_label":"Chapter 2: Voice-to-Action","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: LLMs & Robotics","permalink":"/Physical-AI-And-Humanoid-Robotics-Book/docs/module-four/01-llms-robotics-convergence"},"next":{"title":"Chapter 3: Cognitive Planning","permalink":"/Physical-AI-And-Humanoid-Robotics-Book/docs/module-four/03-cognitive-planning-llms-ros2"}}');var t=i(4848),a=i(8453);const r={id:"02-voice-to-action-whisper",title:"Voice-to-Action: Using OpenAI Whisper for Voice Commands Generation",sidebar_label:"Chapter 2: Voice-to-Action",sidebar_position:2},s="Voice-to-Action: Using OpenAI Whisper for Voice Commands Generation",c={},l=[{value:"Introduction",id:"introduction",level:2},{value:"2.1. Speech-to-Text Overview",id:"21-speech-to-text-overview",level:2},{value:"2.2. OpenAI Whisper API Integration",id:"22-openai-whisper-api-integration",level:2},{value:"Key Features:",id:"key-features",level:3},{value:"Integration Steps (High-Level)",id:"integration-steps-high-level",level:3},{value:"Example Python Script for Whisper API Call",id:"example-python-script-for-whisper-api-call",level:3},{value:"2.3. Processing Voice Commands: Intent Recognition and Basic Parsing",id:"23-processing-voice-commands-intent-recognition-and-basic-parsing",level:2},{value:"Basic Command Parsing",id:"basic-command-parsing",level:3},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"voice-to-action-using-openai-whisper-for-voice-commands-generation",children:"Voice-to-Action: Using OpenAI Whisper for Voice Commands Generation"})}),"\n",(0,t.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsxs)(e.p,{children:['Enabling robots to understand and act upon spoken commands is a significant step towards more intuitive and natural human-robot interaction (HRI). The "Voice-to-Action" pipeline involves two primary stages: ',(0,t.jsx)(e.strong,{children:"Speech-to-Text (STT)"})," to transcribe spoken words into text, and ",(0,t.jsx)(e.strong,{children:"Natural Language Understanding (NLU)"})," to extract actionable intent from that text. OpenAI Whisper is a powerful general-purpose speech recognition model that excels at the STT component, laying the foundation for converting human voice commands into robot actions."]}),"\n",(0,t.jsx)(e.h2,{id:"21-speech-to-text-overview",children:"2.1. Speech-to-Text Overview"}),"\n",(0,t.jsx)(e.p,{children:"Speech-to-Text (STT) technology converts audio input (spoken language) into written text. This is a crucial first step for any voice-controlled system. The accuracy and robustness of the STT model directly impact the robot's ability to correctly interpret commands. Factors like background noise, accents, and multiple speakers can all affect STT performance."}),"\n",(0,t.jsx)(e.h2,{id:"22-openai-whisper-api-integration",children:"2.2. OpenAI Whisper API Integration"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"OpenAI Whisper"})," is a neural network that has been trained on a large dataset of diverse audio and text, making it highly robust to various languages, accents, and background noise. It can perform not only speech recognition but also language identification and speech translation."]}),"\n",(0,t.jsx)(e.h3,{id:"key-features",children:"Key Features:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"High Accuracy"}),": Generally provides excellent transcription quality across a wide range of audio inputs."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Multilingual"}),": Supports transcription in many languages and can translate them into English."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Robustness"}),": Handles background noise and different speaking styles well."]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"integration-steps-high-level",children:"Integration Steps (High-Level)"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Obtain API Key"}),": Access the OpenAI API by obtaining an API key."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Install Client Library"}),": Use the official OpenAI Python client library (",(0,t.jsx)(e.code,{children:"pip install openai"}),")."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Audio Input"}),": Capture audio from a microphone or load an audio file (e.g., WAV, MP3)."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"API Call"}),": Send the audio data to the Whisper API endpoint."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Receive Transcription"}),": Process the API response to get the transcribed text."]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"example-python-script-for-whisper-api-call",children:"Example Python Script for Whisper API Call"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import os\nimport openai\nfrom pydub import AudioSegment # pip install pydub\nfrom pydub.playback import play # pip install simpleaudio (for playback)\n\n# Ensure your OpenAI API key is set as an environment variable\n# os.environ["OPENAI_API_KEY"] = "YOUR_OPENAI_API_KEY"\nopenai.api_key = os.getenv("OPENAI_API_KEY")\n\ndef transcribe_audio(audio_file_path):\n    """\n    Transcribes an audio file using OpenAI Whisper API.\n    """\n    if not os.path.exists(audio_file_path):\n        print(f"Error: Audio file not found at {audio_file_path}")\n        return None\n\n    try:\n        with open(audio_file_path, "rb") as audio_file:\n            transcript = openai.audio.transcriptions.create(\n                model="whisper-1",\n                file=audio_file,\n                response_format="text" # or "json" for more details\n            )\n        return transcript\n    except openai.APIError as e:\n        print(f"OpenAI API Error: {e}")\n        return None\n    except Exception as e:\n        print(f"An unexpected error occurred: {e}")\n        return None\n\ndef main():\n    # --- Example 1: Transcribe a local audio file ---\n    # Create a dummy audio file for demonstration\n    # In a real scenario, this would be actual recorded audio.\n    # For now, we\'ll create a silent WAV for demonstration purposes\n    dummy_audio_file = "dummy_command.wav"\n    AudioSegment.silent(duration=2000).export(dummy_audio_file, format="wav")\n    print(f"Created a dummy audio file: {dummy_audio_file}")\n\n    # Note: For actual testing, you would replace dummy_audio_file with a file\n    # containing spoken commands, e.g., "robot, go forward and pick up the blue ball".\n    \n    # Transcription\n    # In a real application, you\'d feed actual spoken audio here\n    # For this example, we\'ll simulate a transcription for a known command.\n    simulated_transcription = "Robot, go to the kitchen and fetch the coffee mug."\n    print(f"\\nSimulated Whisper Transcription: \'{simulated_transcription}\'")\n\n    # --- Example 2: Actual API call (requires valid audio file and API key) ---\n    # if openai.api_key:\n    #     print("\\nAttempting actual Whisper API call (requires valid audio and API key)...")\n    #     # Replace with your actual audio file containing speech\n    #     # real_audio_file = "my_voice_command.mp3"\n    #     # transcription_from_api = transcribe_audio(real_audio_file)\n    #     # if transcription_from_api:\n    #     #     print(f"Actual Whisper Transcription: \'{transcription_from_api}\'")\n    # else:\n    #     print("\\nSkipping actual Whisper API call: OPENAI_API_KEY not set.")\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Note"}),": To run this example, you need an OpenAI API key and potentially ",(0,t.jsx)(e.code,{children:"pydub"})," and ",(0,t.jsx)(e.code,{children:"simpleaudio"})," for handling audio files. For actual use, you would record audio from a microphone and feed it to the ",(0,t.jsx)(e.code,{children:"transcribe_audio"})," function."]}),"\n",(0,t.jsx)(e.h2,{id:"23-processing-voice-commands-intent-recognition-and-basic-parsing",children:"2.3. Processing Voice Commands: Intent Recognition and Basic Parsing"}),"\n",(0,t.jsxs)(e.p,{children:["Once you have the transcribed text, the next step is to understand the user's ",(0,t.jsx)(e.strong,{children:"intent"})," and extract relevant information (entities, parameters) to form an executable robot command. This is the NLU part of the pipeline."]}),"\n",(0,t.jsx)(e.h3,{id:"basic-command-parsing",children:"Basic Command Parsing"}),"\n",(0,t.jsx)(e.p,{children:"For simple commands, rule-based parsing or regular expressions can be effective. For more complex and flexible commands, Natural Language Processing (NLP) techniques, often powered by smaller LLMs or fine-tuned models, are used."}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# Pseudo-code for basic command parsing\ndef parse_robot_command(text_command: str) -> dict:\n    """\n    Parses a transcribed text command into a structured robot action.\n    """\n    command_lower = text_command.lower()\n    action = {"action": "unknown"}\n\n    if "go to" in command_lower:\n        action["action"] = "navigate"\n        if "kitchen" in command_lower:\n            action["target"] = "kitchen"\n        elif "living room" in command_lower:\n            action["target"] = "living_room"\n        else:\n            action["target"] = "unspecified_location"\n    elif "pick up" in command_lower:\n        action["action"] = "manipulate"\n        if "coffee mug" in command_lower:\n            action["object"] = "coffee_mug"\n        elif "blue ball" in command_lower:\n            action["object"] = "blue_ball"\n        else:\n            action["object"] = "unspecified_object"\n    elif "stop" in command_lower or "halt" in command_lower:\n        action["action"] = "stop"\n    \n    return action\n\ndef main():\n    simulated_transcription = "Robot, please go to the living room."\n    parsed_command = parse_robot_command(simulated_transcription)\n    print(f"Transcribed: \'{simulated_transcription}\'")\n    print(f"Parsed Command: {parsed_command}")\n\n    simulated_transcription_2 = "Hey robot, pick up the blue ball now."\n    parsed_command_2 = parse_robot_command(simulated_transcription_2)\n    print(f"\\nTranscribed: \'{simulated_transcription_2}\'")\n    print(f"Parsed Command: {parsed_command_2}")\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(e.p,{children:"This parsed dictionary can then be used to trigger specific ROS 2 actions or service calls."}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsxs)(e.p,{children:["OpenAI Whisper provides a highly accurate and robust Speech-to-Text solution, forming the initial critical step in a voice-to-action pipeline for robots. By transcribing spoken commands into text, and then applying basic parsing or more advanced NLU techniques, robots can begin to understand and respond to human verbal instructions, paving the way for more natural and intuitive human-robot interaction. The next step is to translate these understood natural language commands into actionable robot plans, which we will explore in ",(0,t.jsx)(e.a,{href:"/Physical-AI-And-Humanoid-Robotics-Book/docs/module-four/03-cognitive-planning-llms-ros2",children:"Chapter 3: Cognitive Planning: Using LLMs to Translate Natural Language into ROS 2 Actions"}),"."]}),"\n",(0,t.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"https://platform.openai.com/docs/guides/speech-to-text",children:"OpenAI Whisper API Documentation"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"https://github.com/openai/openai-python",children:"OpenAI Python Library"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"https://realpython.com/python-speech-recognition/",children:"Speech Recognition with Python"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"https://www.ibm.com/cloud/learn/natural-language-processing",children:"Natural Language Processing (NLP) Basics"})}),"\n"]})]})}function p(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>r,x:()=>s});var o=i(6540);const t={},a=o.createContext(t);function r(n){const e=o.useContext(a);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:r(n.components),o.createElement(a.Provider,{value:e},n.children)}}}]);